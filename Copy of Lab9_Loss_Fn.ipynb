{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyNWqEn/ZZsSFg9GQlo1JdJr"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["\n","# **Mean Squared Error (MSE)**\n","Pros: Simple and widely used for regression problems.\n","Trains faster\n","\n","Cons: Can be sensitive to outliers.\n","\n","Use Cases: Regression tasks.\n"],"metadata":{"id":"HDjD4YJaa_Gn"}},{"cell_type":"code","source":["import tensorflow as tf\n","y_true = tf.constant([3.0, 4.0, 5.0])\n","y_pred = tf.constant([2.5, 4.5, 5.5])\n","loss = tf.reduce_mean(tf.square(y_true - y_pred))\n","mse_loss = tf.keras.losses.MeanSquaredError()\n","loss_value = mse_loss(y_true, y_pred)\n","print(loss_value)"],"metadata":{"id":"RxnJjWQIbDiv","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1692762041059,"user_tz":-330,"elapsed":436,"user":{"displayName":"Philemon Daniel","userId":"06587091607560878110"}},"outputId":"bfc2e036-e24f-4232-bb3d-7bbe9527cd6e"},"execution_count":11,"outputs":[{"output_type":"stream","name":"stdout","text":["tf.Tensor(0.25, shape=(), dtype=float32)\n"]}]},{"cell_type":"markdown","source":["\n","# **Mean Absolute Error (MAE)**\n","\n","Pros: Less sensitive to outliers compared to MSE.\n","\n","Cons: Might not have as smooth a gradient as MSE.\n","\n","Use Cases: Regression tasks, especially when outliers are present.\n","\n"],"metadata":{"id":"Nj8mgPOwbknx"}},{"cell_type":"code","source":["loss = tf.reduce_mean(tf.abs(y_true - y_pred))\n","mae_loss = tf.keras.losses.MeanAbsoluteError()\n","loss_value = mae_loss(y_true, y_pred)\n","print(loss_value)"],"metadata":{"id":"12Sl9qK9bufV","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1692762176581,"user_tz":-330,"elapsed":445,"user":{"displayName":"Philemon Daniel","userId":"06587091607560878110"}},"outputId":"49d0ab4f-01dc-4147-f0cd-3e14b4506c6e"},"execution_count":12,"outputs":[{"output_type":"stream","name":"stdout","text":["tf.Tensor(0.5, shape=(), dtype=float32)\n"]}]},{"cell_type":"markdown","source":["\n","# **Binary cross-entropy**\n","\n","Pros: Less sensitive to outliers compared to MSE.\n","Directly applicable to binary classification problems.\n","Penalizes confident and wrong predictions heavily.\n","\n","Use case: Used for binary classification tasks.\n","As a component in multi-label classification tasks where each label is treated independently.\n"],"metadata":{"id":"6msIhS1QjknC"}},{"cell_type":"markdown","source":["\n","$$\n","- y \\log(p) - (1-y) \\log(1-p)\n","$$\n"],"metadata":{"id":"Svf8wF75jUJK"}},{"cell_type":"code","source":["import tensorflow as tf\n","y_true = tf.constant([0., 1., 1.])\n","y_pred = tf.constant([0.1, 0.8, 0.3])\n","loss = tf.keras.losses.binary_crossentropy(y_true, y_pred)\n","print(loss)"],"metadata":{"id":"gV1_4EvAjVfI","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1692762374033,"user_tz":-330,"elapsed":470,"user":{"displayName":"Philemon Daniel","userId":"06587091607560878110"}},"outputId":"b9d7c521-95ed-4c32-a233-9ffff6117732"},"execution_count":13,"outputs":[{"output_type":"stream","name":"stdout","text":["tf.Tensor(0.5108254, shape=(), dtype=float32)\n"]}]},{"cell_type":"markdown","source":["\n","# **Categorical Cross-Entropy Loss**\n","\n","Cons: Requires one-hot encoding of the labels, which can increase memory usage.\n","Not suitable for multi-label classification\n","\n","Use case: Suitable for multi-class classification problems.\n","# New Section"],"metadata":{"id":"YlvgT4hrQIpq"}},{"cell_type":"markdown","source":["$$\n","−∑\n","i\n","​\n"," Y\n","i\n","​\n"," log(P\n","i\n","​\n"," )\n","$$"],"metadata":{"id":"LegDVtEfPuvH"}},{"cell_type":"code","source":["y_true = tf.constant([[1., 0., 0.], [0., 1., 0.], [0., 0., 1.]])\n","y_pred = tf.constant([[0.7, 0.2, 0.1], [0.1, 0.6, 0.3], [0.2, 0.4, 0.4]])\n","loss = tf.keras.losses.categorical_crossentropy(y_true, y_pred)\n","print(loss)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"uwF0ZeSmRRun","executionInfo":{"status":"ok","timestamp":1692762480023,"user_tz":-330,"elapsed":788,"user":{"displayName":"Philemon Daniel","userId":"06587091607560878110"}},"outputId":"dede9af7-2a9b-473b-fe9f-127f5f9581d3"},"execution_count":14,"outputs":[{"output_type":"stream","name":"stdout","text":["tf.Tensor([0.35667497 0.5108256  0.9162907 ], shape=(3,), dtype=float32)\n"]}]},{"cell_type":"markdown","source":["\n","# **Sparse Categorical Cross-Entropy Loss**\n","\n","Pros: Suitable for multi-class classification problems.\n","Does not require one-hot encoding of the labels, which is memory efficient.\n","Functionally equivalent to categorical cross-entropy but with different label representation.\n","\n","Cons: The label representation might not be as intuitive in some contexts as one-hot encoding.\n"],"metadata":{"id":"gAcfLJQbQp37"}},{"cell_type":"code","source":["y_true = tf.constant([0, 1, 2])\n","y_pred = tf.constant([[0.7, 0.2, 0.1], [0.1, 0.6, 0.3], [0.2, 0.4, 0.4]])\n","loss = tf.keras.losses.sparse_categorical_crossentropy(y_true, y_pred)\n","print(loss)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"JAZxIEaSRNiH","executionInfo":{"status":"ok","timestamp":1692762621532,"user_tz":-330,"elapsed":455,"user":{"displayName":"Philemon Daniel","userId":"06587091607560878110"}},"outputId":"621b1be7-2a52-49c7-b14b-1034ddf458f8"},"execution_count":15,"outputs":[{"output_type":"stream","name":"stdout","text":["tf.Tensor([0.35667497 0.5108256  0.91629076], shape=(3,), dtype=float32)\n"]}]},{"cell_type":"markdown","source":["\n","# **Hinge Loss**\n","\n","Pros: If you're aiming for a decision boundary that completely separates classes without ambiguity, hinge loss can be useful.\n","\n","Cons: While being a benefit, the non-probabilistic nature of hinge loss might not align well with certain tasks where probability estimates are important.\n"],"metadata":{"id":"d-HYr-YlStfA"}},{"cell_type":"markdown","source":["$$\n","Hinge Loss=max(0,1−y⋅f)\n","$$"],"metadata":{"id":"veRGzPKlWJrz"}},{"cell_type":"code","source":["import tensorflow as tf\n","\n","def hinge_loss(y_true, y_pred):\n","    loss = tf.maximum(0., 1. - y_true * y_pred)\n","    mean_loss = tf.reduce_mean(loss)\n","    return mean_loss\n","\n","# Usage example\n","y_true = tf.constant([1, -1, 1], dtype=tf.float32)  # Convert labels to float32\n","y_pred = tf.constant([0.8, -0.7, 0.9], dtype=tf.float32)\n","\n","loss_value = hinge_loss(y_true, y_pred)\n","print(\"Hinge Loss:\", loss_value.numpy())\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Yh48xys5SyMm","executionInfo":{"status":"ok","timestamp":1692762813340,"user_tz":-330,"elapsed":443,"user":{"displayName":"Philemon Daniel","userId":"06587091607560878110"}},"outputId":"c1226363-9bdd-4201-aca1-6e8845079b24"},"execution_count":16,"outputs":[{"output_type":"stream","name":"stdout","text":["Hinge Loss: 0.2\n"]}]},{"cell_type":"code","source":[],"metadata":{"id":"bUW_UJkzVxLF"},"execution_count":null,"outputs":[]}]}