{"nbformat":4,"nbformat_minor":0,"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.6.5"},"colab":{"name":"Copy of Untitled.ipynb","version":"0.3.2","provenance":[{"file_id":"1t3B_dsXOHo-DyZiu30o2cemuQowbBvdl","timestamp":1566536119953}],"collapsed_sections":[]},"accelerator":"GPU"},"cells":[{"cell_type":"code","metadata":{"id":"l_oaqWug0Sgf","colab_type":"code","colab":{}},"source":["import numpy as np\n","np.random.seed(37)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"collapsed":true,"id":"uyncIJ-p0Sgi","colab_type":"code","outputId":"1d67e6e9-27cf-45f7-91be-1ceef1116290","colab":{"base_uri":"https://localhost:8080/","height":34},"executionInfo":{"status":"ok","timestamp":1566536385287,"user_tz":-330,"elapsed":2758,"user":{"displayName":"PHILEMON DANIEL","photoUrl":"","userId":"04120753151914153335"}}},"source":["import keras\n","from keras.datasets import mnist\n","from keras.models import Sequential\n","from keras.layers import Dense\n","from keras.optimizers import SGD"],"execution_count":2,"outputs":[{"output_type":"stream","text":["Using TensorFlow backend.\n"],"name":"stderr"}]},{"cell_type":"code","metadata":{"id":"CRGA9dp70Sgn","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":51},"outputId":"112275df-4e74-4bca-c559-d4d7659c6b53","executionInfo":{"status":"ok","timestamp":1566536386024,"user_tz":-330,"elapsed":3474,"user":{"displayName":"PHILEMON DANIEL","photoUrl":"","userId":"04120753151914153335"}}},"source":["(X_train, y_train), (X_test, y_test) = mnist.load_data()"],"execution_count":3,"outputs":[{"output_type":"stream","text":["Downloading data from https://s3.amazonaws.com/img-datasets/mnist.npz\n","11493376/11490434 [==============================] - 0s 0us/step\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"collapsed":true,"id":"cDzqebRA0Sgp","colab_type":"code","outputId":"117f4b35-5d82-4329-8cf5-0104819ca42a","colab":{"base_uri":"https://localhost:8080/","height":34},"executionInfo":{"status":"ok","timestamp":1566536386025,"user_tz":-330,"elapsed":3462,"user":{"displayName":"PHILEMON DANIEL","photoUrl":"","userId":"04120753151914153335"}}},"source":["X_train.shape"],"execution_count":4,"outputs":[{"output_type":"execute_result","data":{"text/plain":["(60000, 28, 28)"]},"metadata":{"tags":[]},"execution_count":4}]},{"cell_type":"code","metadata":{"collapsed":true,"id":"1MX5WZBy0Sgs","colab_type":"code","outputId":"0e114df5-50d1-4f11-fc39-261701b0c93a","colab":{"base_uri":"https://localhost:8080/","height":34},"executionInfo":{"status":"ok","timestamp":1566536386027,"user_tz":-330,"elapsed":3449,"user":{"displayName":"PHILEMON DANIEL","photoUrl":"","userId":"04120753151914153335"}}},"source":["y_train.shape"],"execution_count":5,"outputs":[{"output_type":"execute_result","data":{"text/plain":["(60000,)"]},"metadata":{"tags":[]},"execution_count":5}]},{"cell_type":"code","metadata":{"collapsed":true,"id":"82NLjl8s0Sgu","colab_type":"code","outputId":"c994ee02-e7f4-4aad-f97f-6359a2a887db","colab":{"base_uri":"https://localhost:8080/","height":1000},"executionInfo":{"status":"ok","timestamp":1566536386028,"user_tz":-330,"elapsed":3434,"user":{"displayName":"PHILEMON DANIEL","photoUrl":"","userId":"04120753151914153335"}}},"source":["X_train[1]"],"execution_count":6,"outputs":[{"output_type":"execute_result","data":{"text/plain":["array([[  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n","          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n","          0,   0],\n","       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n","          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n","          0,   0],\n","       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n","          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n","          0,   0],\n","       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n","          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n","          0,   0],\n","       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n","          0,   0,  51, 159, 253, 159,  50,   0,   0,   0,   0,   0,   0,\n","          0,   0],\n","       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n","          0,  48, 238, 252, 252, 252, 237,   0,   0,   0,   0,   0,   0,\n","          0,   0],\n","       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n","         54, 227, 253, 252, 239, 233, 252,  57,   6,   0,   0,   0,   0,\n","          0,   0],\n","       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,  10,  60,\n","        224, 252, 253, 252, 202,  84, 252, 253, 122,   0,   0,   0,   0,\n","          0,   0],\n","       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0, 163, 252,\n","        252, 252, 253, 252, 252,  96, 189, 253, 167,   0,   0,   0,   0,\n","          0,   0],\n","       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,  51, 238, 253,\n","        253, 190, 114, 253, 228,  47,  79, 255, 168,   0,   0,   0,   0,\n","          0,   0],\n","       [  0,   0,   0,   0,   0,   0,   0,   0,   0,  48, 238, 252, 252,\n","        179,  12,  75, 121,  21,   0,   0, 253, 243,  50,   0,   0,   0,\n","          0,   0],\n","       [  0,   0,   0,   0,   0,   0,   0,   0,  38, 165, 253, 233, 208,\n","         84,   0,   0,   0,   0,   0,   0, 253, 252, 165,   0,   0,   0,\n","          0,   0],\n","       [  0,   0,   0,   0,   0,   0,   0,   7, 178, 252, 240,  71,  19,\n","         28,   0,   0,   0,   0,   0,   0, 253, 252, 195,   0,   0,   0,\n","          0,   0],\n","       [  0,   0,   0,   0,   0,   0,   0,  57, 252, 252,  63,   0,   0,\n","          0,   0,   0,   0,   0,   0,   0, 253, 252, 195,   0,   0,   0,\n","          0,   0],\n","       [  0,   0,   0,   0,   0,   0,   0, 198, 253, 190,   0,   0,   0,\n","          0,   0,   0,   0,   0,   0,   0, 255, 253, 196,   0,   0,   0,\n","          0,   0],\n","       [  0,   0,   0,   0,   0,   0,  76, 246, 252, 112,   0,   0,   0,\n","          0,   0,   0,   0,   0,   0,   0, 253, 252, 148,   0,   0,   0,\n","          0,   0],\n","       [  0,   0,   0,   0,   0,   0,  85, 252, 230,  25,   0,   0,   0,\n","          0,   0,   0,   0,   0,   7, 135, 253, 186,  12,   0,   0,   0,\n","          0,   0],\n","       [  0,   0,   0,   0,   0,   0,  85, 252, 223,   0,   0,   0,   0,\n","          0,   0,   0,   0,   7, 131, 252, 225,  71,   0,   0,   0,   0,\n","          0,   0],\n","       [  0,   0,   0,   0,   0,   0,  85, 252, 145,   0,   0,   0,   0,\n","          0,   0,   0,  48, 165, 252, 173,   0,   0,   0,   0,   0,   0,\n","          0,   0],\n","       [  0,   0,   0,   0,   0,   0,  86, 253, 225,   0,   0,   0,   0,\n","          0,   0, 114, 238, 253, 162,   0,   0,   0,   0,   0,   0,   0,\n","          0,   0],\n","       [  0,   0,   0,   0,   0,   0,  85, 252, 249, 146,  48,  29,  85,\n","        178, 225, 253, 223, 167,  56,   0,   0,   0,   0,   0,   0,   0,\n","          0,   0],\n","       [  0,   0,   0,   0,   0,   0,  85, 252, 252, 252, 229, 215, 252,\n","        252, 252, 196, 130,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n","          0,   0],\n","       [  0,   0,   0,   0,   0,   0,  28, 199, 252, 252, 253, 252, 252,\n","        233, 145,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n","          0,   0],\n","       [  0,   0,   0,   0,   0,   0,   0,  25, 128, 252, 253, 252, 141,\n","         37,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n","          0,   0],\n","       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n","          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n","          0,   0],\n","       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n","          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n","          0,   0],\n","       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n","          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n","          0,   0],\n","       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n","          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n","          0,   0]], dtype=uint8)"]},"metadata":{"tags":[]},"execution_count":6}]},{"cell_type":"code","metadata":{"collapsed":true,"id":"qDrB2AjK0Sgx","colab_type":"code","outputId":"23c7dae9-4158-40d1-cd6f-0997792b4fea","colab":{"base_uri":"https://localhost:8080/","height":102},"executionInfo":{"status":"ok","timestamp":1566536386029,"user_tz":-330,"elapsed":3419,"user":{"displayName":"PHILEMON DANIEL","photoUrl":"","userId":"04120753151914153335"}}},"source":["y_train[:100]"],"execution_count":7,"outputs":[{"output_type":"execute_result","data":{"text/plain":["array([5, 0, 4, 1, 9, 2, 1, 3, 1, 4, 3, 5, 3, 6, 1, 7, 2, 8, 6, 9, 4, 0,\n","       9, 1, 1, 2, 4, 3, 2, 7, 3, 8, 6, 9, 0, 5, 6, 0, 7, 6, 1, 8, 7, 9,\n","       3, 9, 8, 5, 9, 3, 3, 0, 7, 4, 9, 8, 0, 9, 4, 1, 4, 4, 6, 0, 4, 5,\n","       6, 1, 0, 0, 1, 7, 1, 6, 3, 0, 2, 1, 1, 7, 9, 0, 2, 6, 7, 8, 3, 9,\n","       0, 4, 6, 7, 4, 6, 8, 0, 7, 8, 3, 1], dtype=uint8)"]},"metadata":{"tags":[]},"execution_count":7}]},{"cell_type":"code","metadata":{"collapsed":true,"id":"lbdDQhf30Sg0","colab_type":"code","outputId":"f2a6770e-77f9-494d-fc13-b7bff8e418e4","colab":{"base_uri":"https://localhost:8080/","height":34},"executionInfo":{"status":"ok","timestamp":1566536386030,"user_tz":-330,"elapsed":3405,"user":{"displayName":"PHILEMON DANIEL","photoUrl":"","userId":"04120753151914153335"}}},"source":["X_test.shape"],"execution_count":8,"outputs":[{"output_type":"execute_result","data":{"text/plain":["(10000, 28, 28)"]},"metadata":{"tags":[]},"execution_count":8}]},{"cell_type":"code","metadata":{"collapsed":true,"id":"m0DSTv_Y0Sg2","colab_type":"code","outputId":"6e9681da-4305-44f7-d1a8-07330d58237f","colab":{"base_uri":"https://localhost:8080/","height":34},"executionInfo":{"status":"ok","timestamp":1566536386031,"user_tz":-330,"elapsed":3391,"user":{"displayName":"PHILEMON DANIEL","photoUrl":"","userId":"04120753151914153335"}}},"source":["y_test.shape"],"execution_count":9,"outputs":[{"output_type":"execute_result","data":{"text/plain":["(10000,)"]},"metadata":{"tags":[]},"execution_count":9}]},{"cell_type":"code","metadata":{"id":"X663IfwU0Sg5","colab_type":"code","colab":{}},"source":["X_train = X_train.reshape(60000, 784).astype('float32')"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"NpzxTul30Sg8","colab_type":"code","colab":{}},"source":["X_test = X_test.reshape(10000, 784).astype('float32')"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"Zu_8ojo70Sg-","colab_type":"code","colab":{}},"source":["X_train /=255"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"collapsed":true,"id":"LVcWqVm_0ShA","colab_type":"code","outputId":"d82f9e20-7203-4993-ae74-4f479d6f99b6","colab":{"base_uri":"https://localhost:8080/","height":1000},"executionInfo":{"status":"ok","timestamp":1566536386428,"user_tz":-330,"elapsed":3751,"user":{"displayName":"PHILEMON DANIEL","photoUrl":"","userId":"04120753151914153335"}}},"source":["X_train[0]"],"execution_count":13,"outputs":[{"output_type":"execute_result","data":{"text/plain":["array([0.        , 0.        , 0.        , 0.        , 0.        ,\n","       0.        , 0.        , 0.        , 0.        , 0.        ,\n","       0.        , 0.        , 0.        , 0.        , 0.        ,\n","       0.        , 0.        , 0.        , 0.        , 0.        ,\n","       0.        , 0.        , 0.        , 0.        , 0.        ,\n","       0.        , 0.        , 0.        , 0.        , 0.        ,\n","       0.        , 0.        , 0.        , 0.        , 0.        ,\n","       0.        , 0.        , 0.        , 0.        , 0.        ,\n","       0.        , 0.        , 0.        , 0.        , 0.        ,\n","       0.        , 0.        , 0.        , 0.        , 0.        ,\n","       0.        , 0.        , 0.        , 0.        , 0.        ,\n","       0.        , 0.        , 0.        , 0.        , 0.        ,\n","       0.        , 0.        , 0.        , 0.        , 0.        ,\n","       0.        , 0.        , 0.        , 0.        , 0.        ,\n","       0.        , 0.        , 0.        , 0.        , 0.        ,\n","       0.        , 0.        , 0.        , 0.        , 0.        ,\n","       0.        , 0.        , 0.        , 0.        , 0.        ,\n","       0.        , 0.        , 0.        , 0.        , 0.        ,\n","       0.        , 0.        , 0.        , 0.        , 0.        ,\n","       0.        , 0.        , 0.        , 0.        , 0.        ,\n","       0.        , 0.        , 0.        , 0.        , 0.        ,\n","       0.        , 0.        , 0.        , 0.        , 0.        ,\n","       0.        , 0.        , 0.        , 0.        , 0.        ,\n","       0.        , 0.        , 0.        , 0.        , 0.        ,\n","       0.        , 0.        , 0.        , 0.        , 0.        ,\n","       0.        , 0.        , 0.        , 0.        , 0.        ,\n","       0.        , 0.        , 0.        , 0.        , 0.        ,\n","       0.        , 0.        , 0.        , 0.        , 0.        ,\n","       0.        , 0.        , 0.        , 0.        , 0.        ,\n","       0.        , 0.        , 0.        , 0.        , 0.        ,\n","       0.        , 0.        , 0.01176471, 0.07058824, 0.07058824,\n","       0.07058824, 0.49411765, 0.53333336, 0.6862745 , 0.10196079,\n","       0.6509804 , 1.        , 0.96862745, 0.49803922, 0.        ,\n","       0.        , 0.        , 0.        , 0.        , 0.        ,\n","       0.        , 0.        , 0.        , 0.        , 0.        ,\n","       0.        , 0.11764706, 0.14117648, 0.36862746, 0.6039216 ,\n","       0.6666667 , 0.99215686, 0.99215686, 0.99215686, 0.99215686,\n","       0.99215686, 0.88235295, 0.6745098 , 0.99215686, 0.9490196 ,\n","       0.7647059 , 0.2509804 , 0.        , 0.        , 0.        ,\n","       0.        , 0.        , 0.        , 0.        , 0.        ,\n","       0.        , 0.        , 0.        , 0.19215687, 0.93333334,\n","       0.99215686, 0.99215686, 0.99215686, 0.99215686, 0.99215686,\n","       0.99215686, 0.99215686, 0.99215686, 0.9843137 , 0.3647059 ,\n","       0.32156864, 0.32156864, 0.21960784, 0.15294118, 0.        ,\n","       0.        , 0.        , 0.        , 0.        , 0.        ,\n","       0.        , 0.        , 0.        , 0.        , 0.        ,\n","       0.        , 0.07058824, 0.85882354, 0.99215686, 0.99215686,\n","       0.99215686, 0.99215686, 0.99215686, 0.7764706 , 0.7137255 ,\n","       0.96862745, 0.94509804, 0.        , 0.        , 0.        ,\n","       0.        , 0.        , 0.        , 0.        , 0.        ,\n","       0.        , 0.        , 0.        , 0.        , 0.        ,\n","       0.        , 0.        , 0.        , 0.        , 0.        ,\n","       0.3137255 , 0.6117647 , 0.41960785, 0.99215686, 0.99215686,\n","       0.8039216 , 0.04313726, 0.        , 0.16862746, 0.6039216 ,\n","       0.        , 0.        , 0.        , 0.        , 0.        ,\n","       0.        , 0.        , 0.        , 0.        , 0.        ,\n","       0.        , 0.        , 0.        , 0.        , 0.        ,\n","       0.        , 0.        , 0.        , 0.        , 0.05490196,\n","       0.00392157, 0.6039216 , 0.99215686, 0.3529412 , 0.        ,\n","       0.        , 0.        , 0.        , 0.        , 0.        ,\n","       0.        , 0.        , 0.        , 0.        , 0.        ,\n","       0.        , 0.        , 0.        , 0.        , 0.        ,\n","       0.        , 0.        , 0.        , 0.        , 0.        ,\n","       0.        , 0.        , 0.        , 0.        , 0.54509807,\n","       0.99215686, 0.74509805, 0.00784314, 0.        , 0.        ,\n","       0.        , 0.        , 0.        , 0.        , 0.        ,\n","       0.        , 0.        , 0.        , 0.        , 0.        ,\n","       0.        , 0.        , 0.        , 0.        , 0.        ,\n","       0.        , 0.        , 0.        , 0.        , 0.        ,\n","       0.        , 0.        , 0.04313726, 0.74509805, 0.99215686,\n","       0.27450982, 0.        , 0.        , 0.        , 0.        ,\n","       0.        , 0.        , 0.        , 0.        , 0.        ,\n","       0.        , 0.        , 0.        , 0.        , 0.        ,\n","       0.        , 0.        , 0.        , 0.        , 0.        ,\n","       0.        , 0.        , 0.        , 0.        , 0.        ,\n","       0.        , 0.13725491, 0.94509804, 0.88235295, 0.627451  ,\n","       0.42352942, 0.00392157, 0.        , 0.        , 0.        ,\n","       0.        , 0.        , 0.        , 0.        , 0.        ,\n","       0.        , 0.        , 0.        , 0.        , 0.        ,\n","       0.        , 0.        , 0.        , 0.        , 0.        ,\n","       0.        , 0.        , 0.        , 0.        , 0.        ,\n","       0.31764707, 0.9411765 , 0.99215686, 0.99215686, 0.46666667,\n","       0.09803922, 0.        , 0.        , 0.        , 0.        ,\n","       0.        , 0.        , 0.        , 0.        , 0.        ,\n","       0.        , 0.        , 0.        , 0.        , 0.        ,\n","       0.        , 0.        , 0.        , 0.        , 0.        ,\n","       0.        , 0.        , 0.        , 0.        , 0.1764706 ,\n","       0.7294118 , 0.99215686, 0.99215686, 0.5882353 , 0.10588235,\n","       0.        , 0.        , 0.        , 0.        , 0.        ,\n","       0.        , 0.        , 0.        , 0.        , 0.        ,\n","       0.        , 0.        , 0.        , 0.        , 0.        ,\n","       0.        , 0.        , 0.        , 0.        , 0.        ,\n","       0.        , 0.        , 0.        , 0.0627451 , 0.3647059 ,\n","       0.9882353 , 0.99215686, 0.73333335, 0.        , 0.        ,\n","       0.        , 0.        , 0.        , 0.        , 0.        ,\n","       0.        , 0.        , 0.        , 0.        , 0.        ,\n","       0.        , 0.        , 0.        , 0.        , 0.        ,\n","       0.        , 0.        , 0.        , 0.        , 0.        ,\n","       0.        , 0.        , 0.        , 0.9764706 , 0.99215686,\n","       0.9764706 , 0.2509804 , 0.        , 0.        , 0.        ,\n","       0.        , 0.        , 0.        , 0.        , 0.        ,\n","       0.        , 0.        , 0.        , 0.        , 0.        ,\n","       0.        , 0.        , 0.        , 0.        , 0.        ,\n","       0.        , 0.        , 0.        , 0.18039216, 0.50980395,\n","       0.7176471 , 0.99215686, 0.99215686, 0.8117647 , 0.00784314,\n","       0.        , 0.        , 0.        , 0.        , 0.        ,\n","       0.        , 0.        , 0.        , 0.        , 0.        ,\n","       0.        , 0.        , 0.        , 0.        , 0.        ,\n","       0.        , 0.        , 0.        , 0.        , 0.15294118,\n","       0.5803922 , 0.8980392 , 0.99215686, 0.99215686, 0.99215686,\n","       0.98039216, 0.7137255 , 0.        , 0.        , 0.        ,\n","       0.        , 0.        , 0.        , 0.        , 0.        ,\n","       0.        , 0.        , 0.        , 0.        , 0.        ,\n","       0.        , 0.        , 0.        , 0.        , 0.        ,\n","       0.09411765, 0.44705883, 0.8666667 , 0.99215686, 0.99215686,\n","       0.99215686, 0.99215686, 0.7882353 , 0.30588236, 0.        ,\n","       0.        , 0.        , 0.        , 0.        , 0.        ,\n","       0.        , 0.        , 0.        , 0.        , 0.        ,\n","       0.        , 0.        , 0.        , 0.        , 0.        ,\n","       0.        , 0.09019608, 0.25882354, 0.8352941 , 0.99215686,\n","       0.99215686, 0.99215686, 0.99215686, 0.7764706 , 0.31764707,\n","       0.00784314, 0.        , 0.        , 0.        , 0.        ,\n","       0.        , 0.        , 0.        , 0.        , 0.        ,\n","       0.        , 0.        , 0.        , 0.        , 0.        ,\n","       0.        , 0.        , 0.07058824, 0.67058825, 0.85882354,\n","       0.99215686, 0.99215686, 0.99215686, 0.99215686, 0.7647059 ,\n","       0.3137255 , 0.03529412, 0.        , 0.        , 0.        ,\n","       0.        , 0.        , 0.        , 0.        , 0.        ,\n","       0.        , 0.        , 0.        , 0.        , 0.        ,\n","       0.        , 0.        , 0.        , 0.21568628, 0.6745098 ,\n","       0.8862745 , 0.99215686, 0.99215686, 0.99215686, 0.99215686,\n","       0.95686275, 0.52156866, 0.04313726, 0.        , 0.        ,\n","       0.        , 0.        , 0.        , 0.        , 0.        ,\n","       0.        , 0.        , 0.        , 0.        , 0.        ,\n","       0.        , 0.        , 0.        , 0.        , 0.        ,\n","       0.        , 0.53333336, 0.99215686, 0.99215686, 0.99215686,\n","       0.83137256, 0.5294118 , 0.5176471 , 0.0627451 , 0.        ,\n","       0.        , 0.        , 0.        , 0.        , 0.        ,\n","       0.        , 0.        , 0.        , 0.        , 0.        ,\n","       0.        , 0.        , 0.        , 0.        , 0.        ,\n","       0.        , 0.        , 0.        , 0.        , 0.        ,\n","       0.        , 0.        , 0.        , 0.        , 0.        ,\n","       0.        , 0.        , 0.        , 0.        , 0.        ,\n","       0.        , 0.        , 0.        , 0.        , 0.        ,\n","       0.        , 0.        , 0.        , 0.        , 0.        ,\n","       0.        , 0.        , 0.        , 0.        , 0.        ,\n","       0.        , 0.        , 0.        , 0.        , 0.        ,\n","       0.        , 0.        , 0.        , 0.        , 0.        ,\n","       0.        , 0.        , 0.        , 0.        , 0.        ,\n","       0.        , 0.        , 0.        , 0.        , 0.        ,\n","       0.        , 0.        , 0.        , 0.        , 0.        ,\n","       0.        , 0.        , 0.        , 0.        , 0.        ,\n","       0.        , 0.        , 0.        , 0.        , 0.        ,\n","       0.        , 0.        , 0.        , 0.        , 0.        ,\n","       0.        , 0.        , 0.        , 0.        , 0.        ,\n","       0.        , 0.        , 0.        , 0.        , 0.        ,\n","       0.        , 0.        , 0.        , 0.        ], dtype=float32)"]},"metadata":{"tags":[]},"execution_count":13}]},{"cell_type":"code","metadata":{"id":"_Nq3uVAs0ShD","colab_type":"code","colab":{}},"source":["X_test /= 255"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"3HTjZOZ10ShG","colab_type":"code","colab":{}},"source":["n_classes = 10\n","#y_train = keras.utils.to_categorical(y_train, n_classes)\n","#y_test = keras.utils.to_categorical(y_test, n_classes)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"collapsed":true,"id":"pk_12kKT0ShI","colab_type":"code","outputId":"55b0e605-fc8e-4559-c1f7-fb27a3a493cf","colab":{"base_uri":"https://localhost:8080/","height":102},"executionInfo":{"status":"ok","timestamp":1566536386430,"user_tz":-330,"elapsed":3724,"user":{"displayName":"PHILEMON DANIEL","photoUrl":"","userId":"04120753151914153335"}}},"source":["y_train[:100]"],"execution_count":16,"outputs":[{"output_type":"execute_result","data":{"text/plain":["array([5, 0, 4, 1, 9, 2, 1, 3, 1, 4, 3, 5, 3, 6, 1, 7, 2, 8, 6, 9, 4, 0,\n","       9, 1, 1, 2, 4, 3, 2, 7, 3, 8, 6, 9, 0, 5, 6, 0, 7, 6, 1, 8, 7, 9,\n","       3, 9, 8, 5, 9, 3, 3, 0, 7, 4, 9, 8, 0, 9, 4, 1, 4, 4, 6, 0, 4, 5,\n","       6, 1, 0, 0, 1, 7, 1, 6, 3, 0, 2, 1, 1, 7, 9, 0, 2, 6, 7, 8, 3, 9,\n","       0, 4, 6, 7, 4, 6, 8, 0, 7, 8, 3, 1], dtype=uint8)"]},"metadata":{"tags":[]},"execution_count":16}]},{"cell_type":"code","metadata":{"id":"sSECjs240ShK","colab_type":"code","colab":{}},"source":["model = Sequential()"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"M21J9PxL0ShM","colab_type":"code","colab":{}},"source":["model.add(Dense(64, activation='relu', input_shape=(784,)))\n","model.add(Dense(1, activation='relu'))"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"K4azL29q0ShN","colab_type":"code","outputId":"0ea8b77b-29d4-4984-8ff8-900e4a7a328f","colab":{"base_uri":"https://localhost:8080/","height":204},"executionInfo":{"status":"ok","timestamp":1566536538610,"user_tz":-330,"elapsed":684,"user":{"displayName":"PHILEMON DANIEL","photoUrl":"","userId":"04120753151914153335"}}},"source":["model.summary()"],"execution_count":34,"outputs":[{"output_type":"stream","text":["_________________________________________________________________\n","Layer (type)                 Output Shape              Param #   \n","=================================================================\n","dense_7 (Dense)              (None, 64)                50240     \n","_________________________________________________________________\n","dense_8 (Dense)              (None, 1)                 65        \n","=================================================================\n","Total params: 50,305\n","Trainable params: 50,305\n","Non-trainable params: 0\n","_________________________________________________________________\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"CxpwWnay0ShR","colab_type":"code","outputId":"340aa41a-8391-4ac6-be33-e567b0f73482","colab":{"base_uri":"https://localhost:8080/","height":34},"executionInfo":{"status":"ok","timestamp":1566536542058,"user_tz":-330,"elapsed":913,"user":{"displayName":"PHILEMON DANIEL","photoUrl":"","userId":"04120753151914153335"}}},"source":["64*784 + 64"],"execution_count":35,"outputs":[{"output_type":"execute_result","data":{"text/plain":["50240"]},"metadata":{"tags":[]},"execution_count":35}]},{"cell_type":"code","metadata":{"id":"fcHKIL_I0ShT","colab_type":"code","colab":{}},"source":["model.compile(loss='mse', optimizer=SGD(lr=0.1), metrics=['accuracy'])"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"dVHoOSxY0ShW","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":1000},"outputId":"4cf6af06-7a5a-4d7c-d102-f25396e8af60","executionInfo":{"status":"error","timestamp":1566536575618,"user_tz":-330,"elapsed":32496,"user":{"displayName":"PHILEMON DANIEL","photoUrl":"","userId":"04120753151914153335"}}},"source":["model.fit(X_train, y_train, batch_size=10000, epochs=20000, verbose=1, \n","          validation_data=(X_test, y_test))"],"execution_count":37,"outputs":[{"output_type":"stream","text":["Train on 60000 samples, validate on 10000 samples\n","Epoch 1/20000\n","60000/60000 [==============================] - 0s 6us/step - loss: 120.4233 - acc: 0.0795 - val_loss: 28.1290 - val_acc: 0.0980\n","Epoch 2/20000\n","60000/60000 [==============================] - 0s 3us/step - loss: 28.1853 - acc: 0.0987 - val_loss: 28.1290 - val_acc: 0.0980\n","Epoch 3/20000\n","60000/60000 [==============================] - 0s 3us/step - loss: 28.1853 - acc: 0.0987 - val_loss: 28.1290 - val_acc: 0.0980\n","Epoch 4/20000\n","60000/60000 [==============================] - 0s 3us/step - loss: 28.1853 - acc: 0.0987 - val_loss: 28.1290 - val_acc: 0.0980\n","Epoch 5/20000\n","60000/60000 [==============================] - 0s 3us/step - loss: 28.1853 - acc: 0.0987 - val_loss: 28.1290 - val_acc: 0.0980\n","Epoch 6/20000\n","60000/60000 [==============================] - 0s 3us/step - loss: 28.1853 - acc: 0.0987 - val_loss: 28.1290 - val_acc: 0.0980\n","Epoch 7/20000\n","60000/60000 [==============================] - 0s 3us/step - loss: 28.1853 - acc: 0.0987 - val_loss: 28.1290 - val_acc: 0.0980\n","Epoch 8/20000\n","60000/60000 [==============================] - 0s 3us/step - loss: 28.1853 - acc: 0.0987 - val_loss: 28.1290 - val_acc: 0.0980\n","Epoch 9/20000\n","60000/60000 [==============================] - 0s 3us/step - loss: 28.1853 - acc: 0.0987 - val_loss: 28.1290 - val_acc: 0.0980\n","Epoch 10/20000\n","60000/60000 [==============================] - 0s 3us/step - loss: 28.1853 - acc: 0.0987 - val_loss: 28.1290 - val_acc: 0.0980\n","Epoch 11/20000\n","60000/60000 [==============================] - 0s 3us/step - loss: 28.1853 - acc: 0.0987 - val_loss: 28.1290 - val_acc: 0.0980\n","Epoch 12/20000\n","60000/60000 [==============================] - 0s 3us/step - loss: 28.1853 - acc: 0.0987 - val_loss: 28.1290 - val_acc: 0.0980\n","Epoch 13/20000\n","60000/60000 [==============================] - 0s 3us/step - loss: 28.1853 - acc: 0.0987 - val_loss: 28.1290 - val_acc: 0.0980\n","Epoch 14/20000\n","60000/60000 [==============================] - 0s 3us/step - loss: 28.1853 - acc: 0.0987 - val_loss: 28.1290 - val_acc: 0.0980\n","Epoch 15/20000\n","60000/60000 [==============================] - 0s 3us/step - loss: 28.1853 - acc: 0.0987 - val_loss: 28.1290 - val_acc: 0.0980\n","Epoch 16/20000\n","60000/60000 [==============================] - 0s 3us/step - loss: 28.1853 - acc: 0.0987 - val_loss: 28.1290 - val_acc: 0.0980\n","Epoch 17/20000\n","60000/60000 [==============================] - 0s 3us/step - loss: 28.1853 - acc: 0.0987 - val_loss: 28.1290 - val_acc: 0.0980\n","Epoch 18/20000\n","60000/60000 [==============================] - 0s 3us/step - loss: 28.1853 - acc: 0.0987 - val_loss: 28.1290 - val_acc: 0.0980\n","Epoch 19/20000\n","60000/60000 [==============================] - 0s 3us/step - loss: 28.1853 - acc: 0.0987 - val_loss: 28.1290 - val_acc: 0.0980\n","Epoch 20/20000\n","60000/60000 [==============================] - 0s 3us/step - loss: 28.1853 - acc: 0.0987 - val_loss: 28.1290 - val_acc: 0.0980\n","Epoch 21/20000\n","60000/60000 [==============================] - 0s 3us/step - loss: 28.1853 - acc: 0.0987 - val_loss: 28.1290 - val_acc: 0.0980\n","Epoch 22/20000\n","60000/60000 [==============================] - 0s 3us/step - loss: 28.1853 - acc: 0.0987 - val_loss: 28.1290 - val_acc: 0.0980\n","Epoch 23/20000\n","60000/60000 [==============================] - 0s 3us/step - loss: 28.1853 - acc: 0.0987 - val_loss: 28.1290 - val_acc: 0.0980\n","Epoch 24/20000\n","60000/60000 [==============================] - 0s 3us/step - loss: 28.1853 - acc: 0.0987 - val_loss: 28.1290 - val_acc: 0.0980\n","Epoch 25/20000\n","60000/60000 [==============================] - 0s 3us/step - loss: 28.1853 - acc: 0.0987 - val_loss: 28.1290 - val_acc: 0.0980\n","Epoch 26/20000\n","60000/60000 [==============================] - 0s 3us/step - loss: 28.1853 - acc: 0.0987 - val_loss: 28.1290 - val_acc: 0.0980\n","Epoch 27/20000\n","60000/60000 [==============================] - 0s 3us/step - loss: 28.1853 - acc: 0.0987 - val_loss: 28.1290 - val_acc: 0.0980\n","Epoch 28/20000\n","60000/60000 [==============================] - 0s 3us/step - loss: 28.1853 - acc: 0.0987 - val_loss: 28.1290 - val_acc: 0.0980\n","Epoch 29/20000\n","60000/60000 [==============================] - 0s 3us/step - loss: 28.1853 - acc: 0.0987 - val_loss: 28.1290 - val_acc: 0.0980\n","Epoch 30/20000\n","60000/60000 [==============================] - 0s 3us/step - loss: 28.1853 - acc: 0.0987 - val_loss: 28.1290 - val_acc: 0.0980\n","Epoch 31/20000\n","60000/60000 [==============================] - 0s 3us/step - loss: 28.1853 - acc: 0.0987 - val_loss: 28.1290 - val_acc: 0.0980\n","Epoch 32/20000\n","60000/60000 [==============================] - 0s 3us/step - loss: 28.1853 - acc: 0.0987 - val_loss: 28.1290 - val_acc: 0.0980\n","Epoch 33/20000\n","60000/60000 [==============================] - 0s 3us/step - loss: 28.1853 - acc: 0.0987 - val_loss: 28.1290 - val_acc: 0.0980\n","Epoch 34/20000\n","60000/60000 [==============================] - 0s 3us/step - loss: 28.1853 - acc: 0.0987 - val_loss: 28.1290 - val_acc: 0.0980\n","Epoch 35/20000\n","60000/60000 [==============================] - 0s 3us/step - loss: 28.1853 - acc: 0.0987 - val_loss: 28.1290 - val_acc: 0.0980\n","Epoch 36/20000\n","60000/60000 [==============================] - 0s 3us/step - loss: 28.1853 - acc: 0.0987 - val_loss: 28.1290 - val_acc: 0.0980\n","Epoch 37/20000\n","60000/60000 [==============================] - 0s 3us/step - loss: 28.1853 - acc: 0.0987 - val_loss: 28.1290 - val_acc: 0.0980\n","Epoch 38/20000\n","60000/60000 [==============================] - 0s 3us/step - loss: 28.1853 - acc: 0.0987 - val_loss: 28.1290 - val_acc: 0.0980\n","Epoch 39/20000\n","60000/60000 [==============================] - 0s 3us/step - loss: 28.1853 - acc: 0.0987 - val_loss: 28.1290 - val_acc: 0.0980\n","Epoch 40/20000\n","60000/60000 [==============================] - 0s 3us/step - loss: 28.1853 - acc: 0.0987 - val_loss: 28.1290 - val_acc: 0.0980\n","Epoch 41/20000\n","60000/60000 [==============================] - 0s 3us/step - loss: 28.1853 - acc: 0.0987 - val_loss: 28.1290 - val_acc: 0.0980\n","Epoch 42/20000\n","60000/60000 [==============================] - 0s 3us/step - loss: 28.1853 - acc: 0.0987 - val_loss: 28.1290 - val_acc: 0.0980\n","Epoch 43/20000\n","60000/60000 [==============================] - 0s 3us/step - loss: 28.1853 - acc: 0.0987 - val_loss: 28.1290 - val_acc: 0.0980\n","Epoch 44/20000\n","60000/60000 [==============================] - 0s 3us/step - loss: 28.1853 - acc: 0.0987 - val_loss: 28.1290 - val_acc: 0.0980\n","Epoch 45/20000\n","60000/60000 [==============================] - 0s 3us/step - loss: 28.1853 - acc: 0.0987 - val_loss: 28.1290 - val_acc: 0.0980\n","Epoch 46/20000\n","60000/60000 [==============================] - 0s 3us/step - loss: 28.1853 - acc: 0.0987 - val_loss: 28.1290 - val_acc: 0.0980\n","Epoch 47/20000\n","60000/60000 [==============================] - 0s 3us/step - loss: 28.1853 - acc: 0.0987 - val_loss: 28.1290 - val_acc: 0.0980\n","Epoch 48/20000\n","60000/60000 [==============================] - 0s 3us/step - loss: 28.1853 - acc: 0.0987 - val_loss: 28.1290 - val_acc: 0.0980\n","Epoch 49/20000\n","60000/60000 [==============================] - 0s 3us/step - loss: 28.1853 - acc: 0.0987 - val_loss: 28.1290 - val_acc: 0.0980\n","Epoch 50/20000\n","60000/60000 [==============================] - 0s 3us/step - loss: 28.1853 - acc: 0.0987 - val_loss: 28.1290 - val_acc: 0.0980\n","Epoch 51/20000\n","60000/60000 [==============================] - 0s 3us/step - loss: 28.1853 - acc: 0.0987 - val_loss: 28.1290 - val_acc: 0.0980\n","Epoch 52/20000\n","60000/60000 [==============================] - 0s 3us/step - loss: 28.1853 - acc: 0.0987 - val_loss: 28.1290 - val_acc: 0.0980\n","Epoch 53/20000\n","60000/60000 [==============================] - 0s 3us/step - loss: 28.1853 - acc: 0.0987 - val_loss: 28.1290 - val_acc: 0.0980\n","Epoch 54/20000\n","60000/60000 [==============================] - 0s 3us/step - loss: 28.1853 - acc: 0.0987 - val_loss: 28.1290 - val_acc: 0.0980\n","Epoch 55/20000\n","60000/60000 [==============================] - 0s 3us/step - loss: 28.1853 - acc: 0.0987 - val_loss: 28.1290 - val_acc: 0.0980\n","Epoch 56/20000\n","60000/60000 [==============================] - 0s 3us/step - loss: 28.1853 - acc: 0.0987 - val_loss: 28.1290 - val_acc: 0.0980\n","Epoch 57/20000\n","60000/60000 [==============================] - 0s 3us/step - loss: 28.1853 - acc: 0.0987 - val_loss: 28.1290 - val_acc: 0.0980\n","Epoch 58/20000\n","60000/60000 [==============================] - 0s 3us/step - loss: 28.1853 - acc: 0.0987 - val_loss: 28.1290 - val_acc: 0.0980\n","Epoch 59/20000\n","60000/60000 [==============================] - 0s 3us/step - loss: 28.1853 - acc: 0.0987 - val_loss: 28.1290 - val_acc: 0.0980\n","Epoch 60/20000\n","60000/60000 [==============================] - 0s 3us/step - loss: 28.1853 - acc: 0.0987 - val_loss: 28.1290 - val_acc: 0.0980\n","Epoch 61/20000\n","60000/60000 [==============================] - 0s 3us/step - loss: 28.1853 - acc: 0.0987 - val_loss: 28.1290 - val_acc: 0.0980\n","Epoch 62/20000\n","60000/60000 [==============================] - 0s 3us/step - loss: 28.1853 - acc: 0.0987 - val_loss: 28.1290 - val_acc: 0.0980\n","Epoch 63/20000\n","60000/60000 [==============================] - 0s 3us/step - loss: 28.1853 - acc: 0.0987 - val_loss: 28.1290 - val_acc: 0.0980\n","Epoch 64/20000\n","60000/60000 [==============================] - 0s 3us/step - loss: 28.1853 - acc: 0.0987 - val_loss: 28.1290 - val_acc: 0.0980\n","Epoch 65/20000\n","60000/60000 [==============================] - 0s 3us/step - loss: 28.1853 - acc: 0.0987 - val_loss: 28.1290 - val_acc: 0.0980\n","Epoch 66/20000\n","60000/60000 [==============================] - 0s 3us/step - loss: 28.1853 - acc: 0.0987 - val_loss: 28.1290 - val_acc: 0.0980\n","Epoch 67/20000\n","60000/60000 [==============================] - 0s 3us/step - loss: 28.1853 - acc: 0.0987 - val_loss: 28.1290 - val_acc: 0.0980\n","Epoch 68/20000\n","60000/60000 [==============================] - 0s 3us/step - loss: 28.1853 - acc: 0.0987 - val_loss: 28.1290 - val_acc: 0.0980\n","Epoch 69/20000\n","60000/60000 [==============================] - 0s 3us/step - loss: 28.1853 - acc: 0.0987 - val_loss: 28.1290 - val_acc: 0.0980\n","Epoch 70/20000\n","60000/60000 [==============================] - 0s 3us/step - loss: 28.1853 - acc: 0.0987 - val_loss: 28.1290 - val_acc: 0.0980\n","Epoch 71/20000\n","60000/60000 [==============================] - 0s 3us/step - loss: 28.1853 - acc: 0.0987 - val_loss: 28.1290 - val_acc: 0.0980\n","Epoch 72/20000\n","60000/60000 [==============================] - 0s 3us/step - loss: 28.1853 - acc: 0.0987 - val_loss: 28.1290 - val_acc: 0.0980\n","Epoch 73/20000\n","60000/60000 [==============================] - 0s 3us/step - loss: 28.1853 - acc: 0.0987 - val_loss: 28.1290 - val_acc: 0.0980\n","Epoch 74/20000\n","60000/60000 [==============================] - 0s 3us/step - loss: 28.1853 - acc: 0.0987 - val_loss: 28.1290 - val_acc: 0.0980\n","Epoch 75/20000\n","60000/60000 [==============================] - 0s 3us/step - loss: 28.1853 - acc: 0.0987 - val_loss: 28.1290 - val_acc: 0.0980\n","Epoch 76/20000\n","60000/60000 [==============================] - 0s 3us/step - loss: 28.1853 - acc: 0.0987 - val_loss: 28.1290 - val_acc: 0.0980\n","Epoch 77/20000\n","60000/60000 [==============================] - 0s 3us/step - loss: 28.1853 - acc: 0.0987 - val_loss: 28.1290 - val_acc: 0.0980\n","Epoch 78/20000\n","60000/60000 [==============================] - 0s 3us/step - loss: 28.1853 - acc: 0.0987 - val_loss: 28.1290 - val_acc: 0.0980\n","Epoch 79/20000\n","60000/60000 [==============================] - 0s 3us/step - loss: 28.1853 - acc: 0.0987 - val_loss: 28.1290 - val_acc: 0.0980\n","Epoch 80/20000\n","60000/60000 [==============================] - 0s 3us/step - loss: 28.1853 - acc: 0.0987 - val_loss: 28.1290 - val_acc: 0.0980\n","Epoch 81/20000\n","60000/60000 [==============================] - 0s 3us/step - loss: 28.1853 - acc: 0.0987 - val_loss: 28.1290 - val_acc: 0.0980\n","Epoch 82/20000\n","60000/60000 [==============================] - 0s 3us/step - loss: 28.1853 - acc: 0.0987 - val_loss: 28.1290 - val_acc: 0.0980\n","Epoch 83/20000\n","60000/60000 [==============================] - 0s 3us/step - loss: 28.1853 - acc: 0.0987 - val_loss: 28.1290 - val_acc: 0.0980\n","Epoch 84/20000\n","60000/60000 [==============================] - 0s 3us/step - loss: 28.1853 - acc: 0.0987 - val_loss: 28.1290 - val_acc: 0.0980\n","Epoch 85/20000\n","60000/60000 [==============================] - 0s 3us/step - loss: 28.1853 - acc: 0.0987 - val_loss: 28.1290 - val_acc: 0.0980\n","Epoch 86/20000\n","60000/60000 [==============================] - 0s 3us/step - loss: 28.1853 - acc: 0.0987 - val_loss: 28.1290 - val_acc: 0.0980\n","Epoch 87/20000\n","60000/60000 [==============================] - 0s 3us/step - loss: 28.1853 - acc: 0.0987 - val_loss: 28.1290 - val_acc: 0.0980\n","Epoch 88/20000\n","60000/60000 [==============================] - 0s 3us/step - loss: 28.1853 - acc: 0.0987 - val_loss: 28.1290 - val_acc: 0.0980\n","Epoch 89/20000\n","60000/60000 [==============================] - 0s 3us/step - loss: 28.1853 - acc: 0.0987 - val_loss: 28.1290 - val_acc: 0.0980\n","Epoch 90/20000\n","60000/60000 [==============================] - 0s 3us/step - loss: 28.1853 - acc: 0.0987 - val_loss: 28.1290 - val_acc: 0.0980\n","Epoch 91/20000\n","60000/60000 [==============================] - 0s 3us/step - loss: 28.1853 - acc: 0.0987 - val_loss: 28.1290 - val_acc: 0.0980\n","Epoch 92/20000\n","60000/60000 [==============================] - 0s 3us/step - loss: 28.1853 - acc: 0.0987 - val_loss: 28.1290 - val_acc: 0.0980\n","Epoch 93/20000\n","60000/60000 [==============================] - 0s 3us/step - loss: 28.1853 - acc: 0.0987 - val_loss: 28.1290 - val_acc: 0.0980\n","Epoch 94/20000\n","60000/60000 [==============================] - 0s 3us/step - loss: 28.1853 - acc: 0.0987 - val_loss: 28.1290 - val_acc: 0.0980\n","Epoch 95/20000\n","60000/60000 [==============================] - 0s 3us/step - loss: 28.1853 - acc: 0.0987 - val_loss: 28.1290 - val_acc: 0.0980\n","Epoch 96/20000\n","60000/60000 [==============================] - 0s 3us/step - loss: 28.1853 - acc: 0.0987 - val_loss: 28.1290 - val_acc: 0.0980\n","Epoch 97/20000\n","60000/60000 [==============================] - 0s 3us/step - loss: 28.1853 - acc: 0.0987 - val_loss: 28.1290 - val_acc: 0.0980\n","Epoch 98/20000\n","60000/60000 [==============================] - 0s 3us/step - loss: 28.1853 - acc: 0.0987 - val_loss: 28.1290 - val_acc: 0.0980\n","Epoch 99/20000\n","60000/60000 [==============================] - 0s 3us/step - loss: 28.1853 - acc: 0.0987 - val_loss: 28.1290 - val_acc: 0.0980\n","Epoch 100/20000\n","60000/60000 [==============================] - 0s 3us/step - loss: 28.1853 - acc: 0.0987 - val_loss: 28.1290 - val_acc: 0.0980\n","Epoch 101/20000\n","60000/60000 [==============================] - 0s 3us/step - loss: 28.1853 - acc: 0.0987 - val_loss: 28.1290 - val_acc: 0.0980\n","Epoch 102/20000\n","60000/60000 [==============================] - 0s 3us/step - loss: 28.1853 - acc: 0.0987 - val_loss: 28.1290 - val_acc: 0.0980\n","Epoch 103/20000\n","60000/60000 [==============================] - 0s 3us/step - loss: 28.1853 - acc: 0.0987 - val_loss: 28.1290 - val_acc: 0.0980\n","Epoch 104/20000\n","60000/60000 [==============================] - 0s 3us/step - loss: 28.1853 - acc: 0.0987 - val_loss: 28.1290 - val_acc: 0.0980\n","Epoch 105/20000\n","60000/60000 [==============================] - 0s 3us/step - loss: 28.1853 - acc: 0.0987 - val_loss: 28.1290 - val_acc: 0.0980\n","Epoch 106/20000\n","60000/60000 [==============================] - 0s 3us/step - loss: 28.1853 - acc: 0.0987 - val_loss: 28.1290 - val_acc: 0.0980\n","Epoch 107/20000\n","60000/60000 [==============================] - 0s 3us/step - loss: 28.1853 - acc: 0.0987 - val_loss: 28.1290 - val_acc: 0.0980\n","Epoch 108/20000\n","60000/60000 [==============================] - 0s 3us/step - loss: 28.1853 - acc: 0.0987 - val_loss: 28.1290 - val_acc: 0.0980\n","Epoch 109/20000\n","60000/60000 [==============================] - 0s 3us/step - loss: 28.1853 - acc: 0.0987 - val_loss: 28.1290 - val_acc: 0.0980\n","Epoch 110/20000\n","60000/60000 [==============================] - 0s 3us/step - loss: 28.1853 - acc: 0.0987 - val_loss: 28.1290 - val_acc: 0.0980\n","Epoch 111/20000\n","60000/60000 [==============================] - 0s 3us/step - loss: 28.1853 - acc: 0.0987 - val_loss: 28.1290 - val_acc: 0.0980\n","Epoch 112/20000\n","60000/60000 [==============================] - 0s 3us/step - loss: 28.1853 - acc: 0.0987 - val_loss: 28.1290 - val_acc: 0.0980\n","Epoch 113/20000\n","60000/60000 [==============================] - 0s 3us/step - loss: 28.1853 - acc: 0.0987 - val_loss: 28.1290 - val_acc: 0.0980\n","Epoch 114/20000\n","60000/60000 [==============================] - 0s 3us/step - loss: 28.1853 - acc: 0.0987 - val_loss: 28.1290 - val_acc: 0.0980\n","Epoch 115/20000\n","60000/60000 [==============================] - 0s 3us/step - loss: 28.1853 - acc: 0.0987 - val_loss: 28.1290 - val_acc: 0.0980\n","Epoch 116/20000\n","60000/60000 [==============================] - 0s 3us/step - loss: 28.1853 - acc: 0.0987 - val_loss: 28.1290 - val_acc: 0.0980\n","Epoch 117/20000\n","60000/60000 [==============================] - 0s 3us/step - loss: 28.1853 - acc: 0.0987 - val_loss: 28.1290 - val_acc: 0.0980\n","Epoch 118/20000\n","60000/60000 [==============================] - 0s 3us/step - loss: 28.1853 - acc: 0.0987 - val_loss: 28.1290 - val_acc: 0.0980\n","Epoch 119/20000\n","60000/60000 [==============================] - 0s 3us/step - loss: 28.1853 - acc: 0.0987 - val_loss: 28.1290 - val_acc: 0.0980\n","Epoch 120/20000\n","60000/60000 [==============================] - 0s 3us/step - loss: 28.1853 - acc: 0.0987 - val_loss: 28.1290 - val_acc: 0.0980\n","Epoch 121/20000\n","60000/60000 [==============================] - 0s 3us/step - loss: 28.1853 - acc: 0.0987 - val_loss: 28.1290 - val_acc: 0.0980\n","Epoch 122/20000\n","60000/60000 [==============================] - 0s 3us/step - loss: 28.1853 - acc: 0.0987 - val_loss: 28.1290 - val_acc: 0.0980\n","Epoch 123/20000\n","60000/60000 [==============================] - 0s 3us/step - loss: 28.1853 - acc: 0.0987 - val_loss: 28.1290 - val_acc: 0.0980\n","Epoch 124/20000\n","60000/60000 [==============================] - 0s 3us/step - loss: 28.1853 - acc: 0.0987 - val_loss: 28.1290 - val_acc: 0.0980\n","Epoch 125/20000\n","60000/60000 [==============================] - 0s 3us/step - loss: 28.1853 - acc: 0.0987 - val_loss: 28.1290 - val_acc: 0.0980\n","Epoch 126/20000\n","60000/60000 [==============================] - 0s 3us/step - loss: 28.1853 - acc: 0.0987 - val_loss: 28.1290 - val_acc: 0.0980\n","Epoch 127/20000\n","60000/60000 [==============================] - 0s 3us/step - loss: 28.1853 - acc: 0.0987 - val_loss: 28.1290 - val_acc: 0.0980\n","Epoch 128/20000\n","60000/60000 [==============================] - 0s 3us/step - loss: 28.1853 - acc: 0.0987 - val_loss: 28.1290 - val_acc: 0.0980\n","Epoch 129/20000\n","60000/60000 [==============================] - 0s 3us/step - loss: 28.1853 - acc: 0.0987 - val_loss: 28.1290 - val_acc: 0.0980\n","Epoch 130/20000\n","60000/60000 [==============================] - 0s 3us/step - loss: 28.1853 - acc: 0.0987 - val_loss: 28.1290 - val_acc: 0.0980\n","Epoch 131/20000\n","60000/60000 [==============================] - 0s 3us/step - loss: 28.1853 - acc: 0.0987 - val_loss: 28.1290 - val_acc: 0.0980\n","Epoch 132/20000\n","60000/60000 [==============================] - 0s 3us/step - loss: 28.1853 - acc: 0.0987 - val_loss: 28.1290 - val_acc: 0.0980\n","Epoch 133/20000\n","60000/60000 [==============================] - 0s 3us/step - loss: 28.1853 - acc: 0.0987 - val_loss: 28.1290 - val_acc: 0.0980\n","Epoch 134/20000\n","60000/60000 [==============================] - 0s 3us/step - loss: 28.1853 - acc: 0.0987 - val_loss: 28.1290 - val_acc: 0.0980\n","Epoch 135/20000\n","60000/60000 [==============================] - 0s 3us/step - loss: 28.1853 - acc: 0.0987 - val_loss: 28.1290 - val_acc: 0.0980\n","Epoch 136/20000\n","60000/60000 [==============================] - 0s 3us/step - loss: 28.1853 - acc: 0.0987 - val_loss: 28.1290 - val_acc: 0.0980\n","Epoch 137/20000\n","60000/60000 [==============================] - 0s 3us/step - loss: 28.1853 - acc: 0.0987 - val_loss: 28.1290 - val_acc: 0.0980\n","Epoch 138/20000\n","60000/60000 [==============================] - 0s 3us/step - loss: 28.1853 - acc: 0.0987 - val_loss: 28.1290 - val_acc: 0.0980\n","Epoch 139/20000\n","60000/60000 [==============================] - 0s 3us/step - loss: 28.1853 - acc: 0.0987 - val_loss: 28.1290 - val_acc: 0.0980\n","Epoch 140/20000\n","60000/60000 [==============================] - 0s 3us/step - loss: 28.1853 - acc: 0.0987 - val_loss: 28.1290 - val_acc: 0.0980\n","Epoch 141/20000\n","60000/60000 [==============================] - 0s 3us/step - loss: 28.1853 - acc: 0.0987 - val_loss: 28.1290 - val_acc: 0.0980\n","Epoch 142/20000\n","60000/60000 [==============================] - 0s 3us/step - loss: 28.1853 - acc: 0.0987 - val_loss: 28.1290 - val_acc: 0.0980\n","Epoch 143/20000\n","60000/60000 [==============================] - 0s 3us/step - loss: 28.1853 - acc: 0.0987 - val_loss: 28.1290 - val_acc: 0.0980\n","Epoch 144/20000\n","60000/60000 [==============================] - 0s 3us/step - loss: 28.1853 - acc: 0.0987 - val_loss: 28.1290 - val_acc: 0.0980\n","Epoch 145/20000\n","60000/60000 [==============================] - 0s 3us/step - loss: 28.1853 - acc: 0.0987 - val_loss: 28.1290 - val_acc: 0.0980\n","Epoch 146/20000\n","60000/60000 [==============================] - 0s 3us/step - loss: 28.1853 - acc: 0.0987 - val_loss: 28.1290 - val_acc: 0.0980\n","Epoch 147/20000\n","60000/60000 [==============================] - 0s 3us/step - loss: 28.1853 - acc: 0.0987 - val_loss: 28.1290 - val_acc: 0.0980\n","Epoch 148/20000\n","60000/60000 [==============================] - 0s 3us/step - loss: 28.1853 - acc: 0.0987 - val_loss: 28.1290 - val_acc: 0.0980\n","Epoch 149/20000\n","60000/60000 [==============================] - 0s 3us/step - loss: 28.1853 - acc: 0.0987 - val_loss: 28.1290 - val_acc: 0.0980\n","Epoch 150/20000\n","60000/60000 [==============================] - 0s 3us/step - loss: 28.1853 - acc: 0.0987 - val_loss: 28.1290 - val_acc: 0.0980\n","Epoch 151/20000\n","60000/60000 [==============================] - 0s 3us/step - loss: 28.1853 - acc: 0.0987 - val_loss: 28.1290 - val_acc: 0.0980\n","Epoch 152/20000\n","60000/60000 [==============================] - 0s 3us/step - loss: 28.1853 - acc: 0.0987 - val_loss: 28.1290 - val_acc: 0.0980\n","Epoch 153/20000\n","60000/60000 [==============================] - 0s 3us/step - loss: 28.1853 - acc: 0.0987 - val_loss: 28.1290 - val_acc: 0.0980\n","Epoch 154/20000\n","60000/60000 [==============================] - 0s 3us/step - loss: 28.1853 - acc: 0.0987 - val_loss: 28.1290 - val_acc: 0.0980\n","Epoch 155/20000\n","60000/60000 [==============================] - 0s 3us/step - loss: 28.1853 - acc: 0.0987 - val_loss: 28.1290 - val_acc: 0.0980\n","Epoch 156/20000\n","60000/60000 [==============================] - 0s 3us/step - loss: 28.1853 - acc: 0.0987 - val_loss: 28.1290 - val_acc: 0.0980\n","Epoch 157/20000\n","60000/60000 [==============================] - 0s 3us/step - loss: 28.1853 - acc: 0.0987 - val_loss: 28.1290 - val_acc: 0.0980\n","Epoch 158/20000\n","60000/60000 [==============================] - 0s 3us/step - loss: 28.1853 - acc: 0.0987 - val_loss: 28.1290 - val_acc: 0.0980\n","Epoch 159/20000\n","60000/60000 [==============================] - 0s 3us/step - loss: 28.1853 - acc: 0.0987 - val_loss: 28.1290 - val_acc: 0.0980\n","Epoch 160/20000\n","60000/60000 [==============================] - 0s 3us/step - loss: 28.1853 - acc: 0.0987 - val_loss: 28.1290 - val_acc: 0.0980\n","Epoch 161/20000\n","60000/60000 [==============================] - 0s 3us/step - loss: 28.1853 - acc: 0.0987 - val_loss: 28.1290 - val_acc: 0.0980\n","Epoch 162/20000\n","60000/60000 [==============================] - 0s 3us/step - loss: 28.1853 - acc: 0.0987 - val_loss: 28.1290 - val_acc: 0.0980\n","Epoch 163/20000\n","60000/60000 [==============================] - 0s 3us/step - loss: 28.1853 - acc: 0.0987 - val_loss: 28.1290 - val_acc: 0.0980\n","Epoch 164/20000\n","60000/60000 [==============================] - 0s 3us/step - loss: 28.1853 - acc: 0.0987 - val_loss: 28.1290 - val_acc: 0.0980\n","Epoch 165/20000\n","60000/60000 [==============================] - 0s 3us/step - loss: 28.1853 - acc: 0.0987 - val_loss: 28.1290 - val_acc: 0.0980\n","Epoch 166/20000\n","60000/60000 [==============================] - 0s 3us/step - loss: 28.1853 - acc: 0.0987 - val_loss: 28.1290 - val_acc: 0.0980\n","Epoch 167/20000\n","60000/60000 [==============================] - 0s 3us/step - loss: 28.1853 - acc: 0.0987 - val_loss: 28.1290 - val_acc: 0.0980\n","Epoch 168/20000\n","60000/60000 [==============================] - 0s 3us/step - loss: 28.1853 - acc: 0.0987 - val_loss: 28.1290 - val_acc: 0.0980\n","Epoch 169/20000\n","60000/60000 [==============================] - 0s 3us/step - loss: 28.1853 - acc: 0.0987 - val_loss: 28.1290 - val_acc: 0.0980\n","Epoch 170/20000\n","60000/60000 [==============================] - 0s 3us/step - loss: 28.1853 - acc: 0.0987 - val_loss: 28.1290 - val_acc: 0.0980\n","Epoch 171/20000\n","60000/60000 [==============================] - 0s 3us/step - loss: 28.1853 - acc: 0.0987 - val_loss: 28.1290 - val_acc: 0.0980\n","Epoch 172/20000\n","60000/60000 [==============================] - 0s 3us/step - loss: 28.1853 - acc: 0.0987 - val_loss: 28.1290 - val_acc: 0.0980\n","Epoch 173/20000\n","60000/60000 [==============================] - 0s 3us/step - loss: 28.1853 - acc: 0.0987 - val_loss: 28.1290 - val_acc: 0.0980\n","Epoch 174/20000\n","60000/60000 [==============================] - 0s 3us/step - loss: 28.1853 - acc: 0.0987 - val_loss: 28.1290 - val_acc: 0.0980\n","Epoch 175/20000\n","60000/60000 [==============================] - 0s 3us/step - loss: 28.1853 - acc: 0.0987 - val_loss: 28.1290 - val_acc: 0.0980\n","Epoch 176/20000\n","60000/60000 [==============================] - 0s 3us/step - loss: 28.1853 - acc: 0.0987 - val_loss: 28.1290 - val_acc: 0.0980\n","Epoch 177/20000\n","60000/60000 [==============================] - 0s 3us/step - loss: 28.1853 - acc: 0.0987 - val_loss: 28.1290 - val_acc: 0.0980\n","Epoch 178/20000\n","60000/60000 [==============================] - 0s 3us/step - loss: 28.1853 - acc: 0.0987 - val_loss: 28.1290 - val_acc: 0.0980\n","Epoch 179/20000\n","60000/60000 [==============================] - 0s 3us/step - loss: 28.1853 - acc: 0.0987 - val_loss: 28.1290 - val_acc: 0.0980\n","Epoch 180/20000\n","60000/60000 [==============================] - 0s 3us/step - loss: 28.1853 - acc: 0.0987 - val_loss: 28.1290 - val_acc: 0.0980\n","Epoch 181/20000\n","60000/60000 [==============================] - 0s 3us/step - loss: 28.1853 - acc: 0.0987 - val_loss: 28.1290 - val_acc: 0.0980\n","Epoch 182/20000\n","60000/60000 [==============================] - 0s 3us/step - loss: 28.1853 - acc: 0.0987 - val_loss: 28.1290 - val_acc: 0.0980\n","Epoch 183/20000\n","60000/60000 [==============================] - 0s 3us/step - loss: 28.1853 - acc: 0.0987 - val_loss: 28.1290 - val_acc: 0.0980\n","Epoch 184/20000\n","60000/60000 [==============================] - 0s 3us/step - loss: 28.1853 - acc: 0.0987 - val_loss: 28.1290 - val_acc: 0.0980\n","Epoch 185/20000\n","10000/60000 [====>.........................] - ETA: 0s - loss: 27.8830 - acc: 0.0981"],"name":"stdout"},{"output_type":"error","ename":"KeyboardInterrupt","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m<ipython-input-37-485ce161b68e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m model.fit(X_train, y_train, batch_size=10000, epochs=20000, verbose=1, \n\u001b[0;32m----> 2\u001b[0;31m           validation_data=(X_test, y_test))\n\u001b[0m","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, **kwargs)\u001b[0m\n\u001b[1;32m   1037\u001b[0m                                         \u001b[0minitial_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minitial_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1038\u001b[0m                                         \u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1039\u001b[0;31m                                         validation_steps=validation_steps)\n\u001b[0m\u001b[1;32m   1040\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1041\u001b[0m     def evaluate(self, x=None, y=None,\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/engine/training_arrays.py\u001b[0m in \u001b[0;36mfit_loop\u001b[0;34m(model, f, ins, out_labels, batch_size, epochs, verbose, callbacks, val_f, val_ins, shuffle, callback_metrics, initial_epoch, steps_per_epoch, validation_steps)\u001b[0m\n\u001b[1;32m    197\u001b[0m                     \u001b[0mins_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mins_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    198\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 199\u001b[0;31m                 \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    200\u001b[0m                 \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mto_list\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    201\u001b[0m                 \u001b[0;32mfor\u001b[0m \u001b[0ml\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mo\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout_labels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mouts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2713\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_legacy_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2714\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2715\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2716\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2717\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mpy_any\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mis_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2673\u001b[0m             \u001b[0mfetched\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_callable_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0marray_vals\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_metadata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2674\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2675\u001b[0;31m             \u001b[0mfetched\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_callable_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0marray_vals\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2676\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mfetched\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2677\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1456\u001b[0m         ret = tf_session.TF_SessionRunCallable(self._session._session,\n\u001b[1;32m   1457\u001b[0m                                                \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_handle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1458\u001b[0;31m                                                run_metadata_ptr)\n\u001b[0m\u001b[1;32m   1459\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1460\u001b[0m           \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "]}]},{"cell_type":"code","metadata":{"id":"nbyWrv2g0ShZ","colab_type":"code","colab":{}},"source":[""],"execution_count":0,"outputs":[]}]}